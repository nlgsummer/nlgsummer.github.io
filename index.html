<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <link rel="stylesheet" type="text/css" href="style.css">
  <title>NLG Summer School</title>
    
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59578456-1', 'auto');
  ga('send', 'pageview');

</script>
</head>

<body>

  <div id="main">
    <div>
      <img class="banner" src="images/Kin'sCollege.jpg"/>
    </div>


  <h1 id="h1">Natural Language Summer School</h1>
  <h2>27th - 31st July 2015</h2>
  <p>
  The objective of this summer school is to introduce participants to the
  concepts and research questions in natural language generation (NLG),
  summarisation and dialogue systems. Although these three areas produce
  natural language, their distinct communities seldom interact because each
  community relies on different methods and because the inputs to each kind of
  system are different. There is, however, considerable overlap in the kinds of
  problems that need to be considered, from selecting the right content to
  evaluating the systems. We believe that focusing on the similarities of the
  different areas can stimulate "cross-pollination" of research. For example,
  most summarisation techniques could benefit from deeper semantic processing as
  performed in dialogue systems. Similarly, many NLG systems could benefit from
  techniques used by dialogue systems to substantially improve the generated
  output.
</p>

<h2>Summer School Time Table</h2>

<table>
  <tr>
    <th> Time</th>
    <th> Monday</th>
    <th> Tuesday</th>
    <th> Wednesday</th>
    <th> Thursday</th>
    <th> Friday</th>
  </tr>
  <tr>
    <td> 9:00-10:30</td>
    <td class="col"> Intro to NLG <br><span class="lec">(Ehud Reiter)</span></td>
    <td class="col"> Learning to Generate <br><span class="lec">Yannis Konstas</span></td>
    <td class="col"> Intro to Summarisation <br><span class="lec">Advaith Siddharthan</span></td>
    <td class="col"> System Evaluation <br><span class="lec">Ehud Reiter</span></td>
    <td class="col"> Readability <br><span class="lec">Thomas François</span></td>
  </tr>
  <tr>
      <td class="break">10:30-10:45</td>
      <td class="break">break</td>
      <td class="break">break</td>
      <td class="break">break</td>
      <td class="break">break</td>
      <td class="break">break</td>
  </tr>
  <tr>
    <td> 10:45-12:15</td>
    <td class="col"> NLG in Detail (Content Determination)<br><span class="lec">Chris Mellish</span></td>
    <td class="col"> Intro to Dialogue Systems <br><span class="lec">Paul Piwek</span></td>
    <td class="col"> Intro to Summarisation<br><span class="lec">Advaith Siddharthan</span></td>
    <td class="col"> Cognitive Modelling <br><span class="lec">Kees van Deemter</span></td>
    <td class="col"> Readability<br><span class="lec">Thomas François</span></td>
  </tr>
  <tr>
      <td class="break">12:15:13:00</td>
      <td class="break">lunch</td>
      <td class="break">lunch</td>
      <td class="break">lunch</td>
      <td class="break">lunch</td>
      <td class="break">lunch</td>
  </tr>
  <tr>
    <td> 13:00-14:30</td>
    <td class="col"> NLG in Detail (Micro Planning)<br><span class="lec">Albert Gatt</span></td>
    <td class="col">Intro to Dialogue Systems <br><span class="lec">Paul Piwek</span></td>
    <td class="col"> NLG in Practice<br><span class="lec">Yaji Sripada</span></td>
    <td class="col"> <br><span class="lec">(TBC)</span></td>
    <td class="col"> <br><span class="lec">(TBC)</span></td>
  </tr>
  <tr>
      <td class="break">14:30-14:45</td>
      <td class="break">break</td>
      <td class="break">break</td>
      <td class="break">break</td>
      <td class="break">break</td>
      <td class="break">break</td>
  </tr>
  <tr>
    <td> 14:45-16:15</td>
    <td class="col"> NLG in Detail (Surface Realisation)<br><span class="lec">Albert Gatt</span></td>
    <td class="col" rowspan="3">Open Lab <br><span class="lec">Yaji Sripada</span></td>
    <td class="col" rowspan="3">Open Lab <br><span class="lec">Yaji Sripada</span></td>
    <td class="col" rowspan="3">Open Lab <br><span class="lec">Yaji Sripada</span></td>
    <td class="col"> Discussion </td>
  </tr>
      <tr>
      <td class="break">16:15-16:30</td>
      <td class="break">break</td>
      <td class="end" rowspan="4">end</td>
    </tr>
    <tr>
      <td >16:30-18:00</td>
      <td class="col">Open Lab <br><span class="lec">Yaji Sripada</span></td>
    </tr>
    <tr>
      <td class="break">18:00-18:30</td>
      <td class="break" rowspan="2">Social Event</td>
      <td class="break">break</td>
      <td class="break">break</td>
      <td class="break">break</td>
    </tr>
    <tr>
        <td >18:30-19:30</td>
        <td class="col"> Evening Lecture<br><span class="lec">Paul Piwek</span></td>
        <td class="col"> Evening Lecture<br><span class="lec">Graham Ritchie</span></td>
        <td class="col"> Evening Lecture<br><span class="lec">(TBC)</span></td>
    </tr>
</table>

<h2>Course Summary</h2>
<ol>
    <li><span class="title">Introduction to NLG: </span><br>
        The basic concepts of NLG will be introduced, including document planning, microplanning,           and realisation.  Also examples of real NLG systems will be presented, including both what           they do and how they work.</li>
    <li>
    <span class="title"> NLG in detail </span><ul>
         <li><span class="title">Content Determination:</span></li>
        This talk will summarise existing approaches to content determination for NLG, as well as touching on the closely related topics of text ordering and structuring.  It will discuss why content determination is hard and what sorts of (hand-crafted or learned) models can be used to inform it. It is often useful to regard content determination as a search problem, and we will take this approach in order to compare the different methods that have been used.
         <li><span class="title">Micro-planning:</span></li>
         <li><span class="title">Surface Realisation:</span></li>
        
        </ul>
    </li>
    <li><span class="title">Introduction to Summarisation:</span>

    </li>
    <li><span class="title">Introduction to Dialogue Systems: </span><br>
The module will start with a question: What is dialogue? We will survey studies on human-human dialogue in search for a tentative answer. We then compare and contrast human-human dialogue with human-machine dialogue. This will lead us to an examination of various dialogue systems - past and present. We will consider dialogue system architectures and approaches to dialogue management. The module concludes with a look at recent developments, from incremental processing in dialogue to models of non-cooperative dialogue.
    </li>
    <li> <span class="title">Learning to generate: Concept-to-text generation using machine learning </span><br>
        The explosion of the world wide web in the recent years has generated data that are both in very large amounts, as well is in obscure or inaccessible formats to
non-expert users (e.g., numeric, tabular forms, graphs, etc).
Several successful generation systems have been produced in the past twenty years, that mostly rely on human-crafted rules or expert-driven grammars.
While reliable and capable of generating high-quality output, such systems are usually difficult to exploit patterns in very large sets of data, as well as 
port to different domains, without significant human intervention.
In this tutorial, we will explore systems in NLG that learn the well-known pipeline modules of content selection, microplanning and surface realisation,
automatically from data. We will visit methods that model each step into a probabilistic model or other weighted function, and learn their parameters by optimising
a text output-related objective metric (e.g., BLEU, METEOR scores). Generation is then viewed as a common search problem, which entails finding the best 
combination of parameters given the trained model and an input. We will also compare systems that optimise each module in isolation, as well as jointly.
    </li>
    <li> <span class="title">Evaluation: </span><br>
         Different ways to evaluate NLG systems will be discussed, including task-based, human-              based, and metric-based; what can we learn from these different types of evaluation?  We            will also summarise practice in each of these types of evaluations, and present the design          and outcomes of several real NLG evaluations.

    </li>
    <li> <span class="title">Cognitive Modelling:</span>
    </li>
    <li> <span class="title">Readability:</span>
    </li>
    <li>
    <span class="title">Open Lab: </span>
    </li>
    <li> <span class="title">Evening lectures:</span>
        <ul>
        <li><span class="title">Creative Language Generation </span> by <span class="lec">Graeme Ritchie</span> <br>
        Over the past two decades, the field of computational creativity has come into being, and grown considerably. Its aim is "to model, simulate or replicate creativity using a computer". To this end, most of the work focusses on building software which engages in activities commonly thought, within society, to be creative, such as visual art, concept generation, musical composition, etc. Many of these domains involve natural language (e.g. poetry, stories, jokes), and hence such work can be seen as a subarea, creative language generation. This talk will give a general introduction to the field of computational creativity, briefly review some of the language-based work, and discuss some of the methodological issues raised by such research.
    
        </li>
        </ul>
    </li>
</ol>

    <h2>Sponsors </h2>  
      <div id="sponsor">
        <img src="images/SICSA%20logo.png" class="logo"/>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <img src="images/Aberdeen_Uni_logo.jpg" class="logo"/>
      </div>

 <br>
<hr>
    <footer>
    <p>Send us an <a href="mailto:nlg.summer@abdn.ac.uk">email</a>  if you have a question.</p>
      <p>Last update: 13th February 2015</p>
    </footer>

 </div>
</body>
</html>
